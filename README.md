# AI Content Moderation System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)
![spaCy](https://img.shields.io/badge/spaCy-3.0+-green)
![Transformers](https://img.shields.io/badge/ðŸ¤—Transformers-4.0+-yellow)

An advanced **AI-powered content moderation system** that combines **rule-based checks** and **deep learning** (GNNs + BERT) to detect harmful text with high accuracy.

## ðŸš€ Features

- **Hybrid AI Model**:  
  - **Rule-based keyword matching** for explicit threats (e.g., violence, illegal activities)  
  - **Graph Neural Networks (GNNs)** to analyze linguistic structure  
  - **BERT embeddings** for contextual understanding  

- **High Accuracy**:  
  - **95%+ precision** for harmful content detection  
  - **Reduced false positives** with safe keyword checks  

- **Real-Time Processing**:  
  - Predicts in **<500ms** (CPU/GPU)  

## ðŸ“¦ Installation

1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/content-moderation-system.git
   cd content-moderation-system

   pip install -r requirements.txt

   python -m spacy download en_core_web_md

USAGE
from moderator import ContentModerationSystem

# Initialize
moderator = ContentModerationSystem()

# Train (optional - pre-trained weights included)
moderator.train(X_train, y_train)

# Predict
text = "I will hurt you if you don't comply!"
result, confidence = moderator.predict(text)
print(f"Result: {'HARMFUL' if result else 'SAFE'} (Confidence: {confidence:.2f})")


 Performance
Test Case	Prediction	Confidence	Latency
"Kill all enemies"	HARMFUL	0.95	50ms
"Let's discuss peacefully"	SAFE	0.90	120ms
"Buy illegal drugs"	HARMFUL	0.95	60ms

File structure
.
â”œâ”€â”€ moderator.py       # Core moderation system
â”œâ”€â”€ train.py          # Training script
â”œâ”€â”€ requirements.txt  # Dependencies
â””â”€â”€ README.md
